{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install brevitas\n",
        "!pip install \"setuptools<70.0\"\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzgL32BODz-D",
        "outputId": "2e6af282-298f-40d5-f24f-ee586acf0216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: brevitas in /usr/local/lib/python3.11/dist-packages (0.12.0)\n",
            "Requirement already satisfied: dependencies==2.0.1 in /usr/local/lib/python3.11/dist-packages (from brevitas) (2.0.1)\n",
            "Requirement already satisfied: numpy<=1.26.4 in /usr/local/lib/python3.11/dist-packages (from brevitas) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from brevitas) (24.2)\n",
            "Requirement already satisfied: setuptools<70.0 in /usr/local/lib/python3.11/dist-packages (from brevitas) (69.5.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from brevitas) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from brevitas) (2.6.0+cu124)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from brevitas) (4.13.2)\n",
            "Requirement already satisfied: unfoldNd in /usr/local/lib/python3.11/dist-packages (from brevitas) (0.2.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->brevitas) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->brevitas) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9.1->brevitas) (3.0.2)\n",
            "Requirement already satisfied: setuptools<70.0 in /usr/local/lib/python3.11/dist-packages (69.5.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFD9TzO6ObK_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertModel, DistilBertForSequenceClassification\n",
        "from brevitas.nn import QuantLinear, QuantEmbedding, QuantReLU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantDistilBertSelfAttention(nn.Module):\n",
        "    def __init__(self, config, bit_width):\n",
        "        super().__init__()\n",
        "        self.num_heads = config.n_heads\n",
        "        self.head_dim = config.dim // config.n_heads\n",
        "\n",
        "        self.q_lin = QuantLinear(config.dim, config.dim, bias=True, weight_bit_width=bit_width)\n",
        "        self.k_lin = QuantLinear(config.dim, config.dim, bias=True, weight_bit_width=bit_width)\n",
        "        self.v_lin = QuantLinear(config.dim, config.dim, bias=True, weight_bit_width=bit_width)\n",
        "        self.out_lin = QuantLinear(config.dim, config.dim, bias=True, weight_bit_width=bit_width)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask):\n",
        "        batch_size, seq_length, dim = query.size()\n",
        "\n",
        "        q = self.q_lin(query).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.k_lin(key).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.v_lin(value).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        if mask is not None:\n",
        "            extended_mask = mask.unsqueeze(1).unsqueeze(2)\n",
        "            scores = scores.masked_fill(extended_mask == 0, float('-inf'))\n",
        "        weights = torch.softmax(scores, dim=-1)\n",
        "        weights = self.dropout(weights)\n",
        "\n",
        "        context = torch.matmul(weights, v)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_length, dim)\n",
        "        output = self.out_lin(context)\n",
        "        return output"
      ],
      "metadata": {
        "id": "lzHI96reOl-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantDistilBertFeedForward(nn.Module):\n",
        "    def __init__(self, config, bit_width):\n",
        "        super().__init__()\n",
        "        self.lin1 = QuantLinear(config.dim, config.hidden_dim, bias=True, weight_bit_width=bit_width)\n",
        "        self.lin2 = QuantLinear(config.hidden_dim, config.dim, bias=True, weight_bit_width=bit_width)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.activation = QuantReLU(bit_width=bit_width, return_quant_tensor=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.lin2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "J-NvE-nmztNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantDistilBertLayer(nn.Module):\n",
        "    def __init__(self, config, bit_width):\n",
        "        super().__init__()\n",
        "        self.attention = QuantDistilBertSelfAttention(config, bit_width)\n",
        "        self.sa_layer_norm = nn.LayerNorm(config.dim)\n",
        "        self.ff = QuantDistilBertFeedForward(config, bit_width)\n",
        "        self.output_layer_norm = nn.LayerNorm(config.dim)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        sa_output = self.attention(x, x, x, mask)\n",
        "        x = self.sa_layer_norm(x + self.dropout(sa_output))\n",
        "        ff_output = self.ff(x)\n",
        "        x = self.output_layer_norm(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "T6kxVzwVzubq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantDistilBert(nn.Module):\n",
        "    def __init__(self, config, bit_width):\n",
        "        super().__init__()\n",
        "        self.embeddings = QuantEmbedding(config.vocab_size, config.dim, padding_idx=config.pad_token_id, weight_bit_width=bit_width)\n",
        "        self.position_embeddings = QuantEmbedding(config.max_position_embeddings, config.dim, weight_bit_width=bit_width)\n",
        "        self.token_type_embeddings = QuantEmbedding(2, config.dim, weight_bit_width=bit_width)\n",
        "        self.emb_dropout = nn.Dropout(config.dropout)\n",
        "        self.emb_layer_norm = nn.LayerNorm(config.dim)\n",
        "\n",
        "        self.transformer = nn.ModuleList([\n",
        "            QuantDistilBertLayer(config, bit_width) for _ in range(config.n_layers)\n",
        "        ])\n",
        "\n",
        "        self.pre_classifier = QuantLinear(config.dim, config.dim, bias=True, weight_bit_width=bit_width)\n",
        "        self.classifier = QuantLinear(config.dim, config.num_labels, bias=True, weight_bit_width=bit_width)\n",
        "        self.activation = QuantReLU(bit_width=bit_width, return_quant_tensor=False)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = torch.arange(0, seq_length, dtype=torch.long, device=input_ids.device).unsqueeze(0).expand_as(input_ids)\n",
        "        token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        word_embeddings = self.embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = word_embeddings + position_embeddings + token_type_embeddings\n",
        "        embeddings = self.emb_layer_norm(self.emb_dropout(embeddings))\n",
        "\n",
        "        hidden_state = embeddings\n",
        "        for layer in self.transformer:\n",
        "            hidden_state = layer(hidden_state, attention_mask)\n",
        "\n",
        "        pooled_output = hidden_state[:, 0]\n",
        "        pooled_output = self.pre_classifier(pooled_output)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "CfkzSWkmzx8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transfer_weights(pretrained, quant):\n",
        "    quant.embeddings.weight.data = pretrained.embeddings.word_embeddings.weight.data.clone()\n",
        "    quant.position_embeddings.weight.data = pretrained.embeddings.position_embeddings.weight.data.clone()\n",
        "\n",
        "    if hasattr(pretrained.embeddings, \"token_type_embeddings\"):\n",
        "        quant.token_type_embeddings.weight.data = pretrained.embeddings.token_type_embeddings.weight.data.clone()\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            quant.token_type_embeddings.weight.zero_()\n",
        "    #tranform layer\n",
        "    quant.emb_layer_norm.load_state_dict(pretrained.embeddings.LayerNorm.state_dict())\n",
        "\n",
        "    for q_layer, p_layer in zip(quant.transformer, pretrained.transformer.layer):\n",
        "        q_layer.attention.q_lin.weight.data = p_layer.attention.q_lin.weight.data.clone()\n",
        "        q_layer.attention.q_lin.bias.data = p_layer.attention.q_lin.bias.data.clone()\n",
        "        q_layer.attention.k_lin.weight.data = p_layer.attention.k_lin.weight.data.clone()\n",
        "        q_layer.attention.k_lin.bias.data = p_layer.attention.k_lin.bias.data.clone()\n",
        "        q_layer.attention.v_lin.weight.data = p_layer.attention.v_lin.weight.data.clone()\n",
        "        q_layer.attention.v_lin.bias.data = p_layer.attention.v_lin.bias.data.clone()\n",
        "        q_layer.attention.out_lin.weight.data = p_layer.attention.out_lin.weight.data.clone()\n",
        "        q_layer.attention.out_lin.bias.data = p_layer.attention.out_lin.bias.data.clone()\n",
        "        # feedforward\n",
        "        q_layer.ff.lin1.weight.data = p_layer.ffn.lin1.weight.data.clone()\n",
        "        q_layer.ff.lin1.bias.data = p_layer.ffn.lin1.bias.data.clone()\n",
        "        q_layer.ff.lin2.weight.data = p_layer.ffn.lin2.weight.data.clone()\n",
        "        q_layer.ff.lin2.bias.data = p_layer.ffn.lin2.bias.data.clone()\n",
        "        #layernorm\n",
        "        q_layer.sa_layer_norm.load_state_dict(p_layer.sa_layer_norm.state_dict())\n",
        "        q_layer.output_layer_norm.load_state_dict(p_layer.output_layer_norm.state_dict())"
      ],
      "metadata": {
        "id": "UE8t4ZTJG09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    config = DistilBertConfig.from_pretrained(\"distilbert-base-uncased\")\n",
        "    config.num_labels = 2\n",
        "    bit_width = 4\n",
        "\n",
        "    pretrained_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "    quant_model = QuantDistilBert(config, bit_width=bit_width)\n",
        "    transfer_weights(pretrained_model, quant_model)\n",
        "    print(\"Pretrained weights transferred to quantized model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHg0SNeZz5qE",
        "outputId": "b8426413-7d99-4d5e-a44b-9acd357cf662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained weights transferred to quantized model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "vFQUUY5FIn7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzSMOaxpIqhd",
        "outputId": "55571e34-07e3-4c80-dd20-ba4b2cc794f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/tweetdataset.csv\")\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words and word.isalpha()]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['text'] = df['text'].astype(str).apply(preprocess)\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['label'] = le.fit_transform(df['sentiment'])"
      ],
      "metadata": {
        "id": "PWfKKIyWIrIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "YsUeyxCiQ4j7",
        "outputId": "85051e35-994a-40c8-9f9b-02cd82ef0ade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "1    12547\n",
              "2     9685\n",
              "0     8782\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8782</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "target_count = 8500\n",
        "\n",
        "df_label_0 = df[df['label'] == 0].sample(n=min(target_count, df['label'].value_counts()[0]), random_state=42)\n",
        "df_label_1 = df[df['label'] == 1].sample(n=min(target_count, df['label'].value_counts()[1]), random_state=42)\n",
        "df_label_2 = df[df['label'] == 2].sample(n=min(target_count, df['label'].value_counts()[2]), random_state=42)\n",
        "\n",
        "df_balanced = pd.concat([df_label_0, df_label_1, df_label_2]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(df_balanced['label'].value_counts())\n",
        "print(f\"Total size of balanced dataframe: {len(df_balanced)}\")\n",
        "\n",
        "df = df_balanced\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2q5fY74KTJxr",
        "outputId": "b57c4d25-3dd8-45bf-84d9-4acc0bc0bf7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "0    8500\n",
            "1    8500\n",
            "2    8500\n",
            "Name: count, dtype: int64\n",
            "Total size of balanced dataframe: 25500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        enc = tokenizer(self.texts[idx], truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n",
        "        return {\n",
        "            'input_ids': enc['input_ids'].squeeze(),\n",
        "            'attention_mask': enc['attention_mask'].squeeze(),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "ysv25CXBIusr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(df['text'].tolist(), df['label'].tolist(), test_size=0.1,random_state=42)\n",
        "\n",
        "train_dataset = TweetDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = TweetDataset(val_texts, val_labels, tokenizer)\n"
      ],
      "metadata": {
        "id": "hVF6YsExIyCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_labels = len(le.classes_)\n",
        "\n",
        "fp_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)\n",
        "fp_model.to(device)\n",
        "optimizer = torch.optim.AdamW(fp_model.parameters(), lr=2e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "for epoch in range(5):\n",
        "    fp_model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader, desc=f\"FP Epoch {epoch+1}\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = fp_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"FP Epoch {epoch+1}, Training Loss: {total_loss / len(train_loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdk0UGnlI9xN",
        "outputId": "64cce710-c50f-4298-aef1-ea618451fc44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "FP Epoch 1: 100%|██████████| 1435/1435 [04:00<00:00,  5.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP Epoch 1, Training Loss: 0.6724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FP Epoch 2: 100%|██████████| 1435/1435 [04:00<00:00,  5.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP Epoch 2, Training Loss: 0.5245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FP Epoch 3: 100%|██████████| 1435/1435 [03:59<00:00,  6.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP Epoch 3, Training Loss: 0.3930\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FP Epoch 4: 100%|██████████| 1435/1435 [04:01<00:00,  5.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP Epoch 4, Training Loss: 0.2593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FP Epoch 5: 100%|██████████| 1435/1435 [04:01<00:00,  5.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP Epoch 5, Training Loss: 0.1658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fp_save_path = \"/content/finetuned_distilbert.pth\"\n",
        "torch.save(fp_model.state_dict(), fp_save_path)\n",
        "print(f\"Full-precision fine-tuned model saved to {fp_save_path}\")"
      ],
      "metadata": {
        "id": "3KBI612cUnsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bit_widths = [1, 4, 8, 32]\n",
        "qat_epochs = 3 #bisa naikin, lagi coba qat 3 epoch 5\n",
        "models = {}"
      ],
      "metadata": {
        "id": "4zVbufMKJ-CI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = DistilBertConfig.from_pretrained(\"distilbert-base-uncased\")\n",
        "config.num_labels = num_labels\n",
        "\n",
        "for bit in bit_widths:\n",
        "    print(f\"QAT {bit}-bit\")\n",
        "    quant_model = QuantDistilBert(config, bit_width=bit)\n",
        "    base_fp_model = DistilBertModel(config)\n",
        "    fp_state_dict = torch.load(fp_save_path, map_location='cpu')\n",
        "    encoder_state_dict = {k.replace(\"distilbert.\", \"\"): v for k, v in fp_state_dict.items() if k.startswith(\"distilbert.\")}\n",
        "    base_fp_model.load_state_dict(encoder_state_dict, strict=False)\n",
        "\n",
        "    transfer_weights(base_fp_model, quant_model)\n",
        "    quant_model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(quant_model.parameters(), lr=2e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for epoch in range(qat_epochs):\n",
        "        quant_model.train()\n",
        "        total_loss = 0\n",
        "        for batch in tqdm(train_loader, desc=f\"QAT {bit}-bit Epoch {epoch+1}\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = quant_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"QAT {bit}-bit Epoch {epoch+1}, Training Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    save_path = f\"/content/quant_distilbert_{bit}bit.pth\"\n",
        "    torch.save(quant_model.state_dict(), save_path)\n",
        "    models[bit] = quant_model\n",
        "    print(f\"QAT {bit}-bit model saved to {save_path}\")"
      ],
      "metadata": {
        "id": "-PIpgloNU2xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\Evaluation\")\n",
        "for bit, model in models.items():\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    print(f\"Accuracy for {bit}bit QAT model: {acc:.4f}\")"
      ],
      "metadata": {
        "id": "MpdxztMNU6ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nO8wgZxRViXo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}